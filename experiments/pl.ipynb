{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.hub\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from PIL import Image\n",
    "sys.path.append('../../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categories():\n",
    "\ttxt_path = os.path.join('../../../interrogate', 'flavors.txt')\n",
    "\twith open(txt_path, 'r', encoding='utf-8') as f:\n",
    "\t\treturn [line.strip() for line in f.readlines()]\n",
    "c = categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "device = 'cuda'\n",
    "clip_model_name = 'ViT-L/14'\n",
    "clip_models_path = '/f/stablediffusion/stable-diffusion-webui/models/clip-interrogator'\n",
    "blip_image_eval_size = 384\n",
    "dtype = torch.float32\n",
    "\n",
    "# from modules/interrogate.py\n",
    "def load_clip_model():\n",
    "\timport clip\n",
    "\tmodel, preprocess = clip.load(clip_model_name)\n",
    "\tmodel.eval()\n",
    "\tmodel = model.to(device)\n",
    "\treturn model, preprocess\n",
    "\n",
    "def preprocess_img(preprocess, pil_image):\n",
    "\t\tclip_image = preprocess(pil_image).unsqueeze(0).type(dtype).to(device)\n",
    "\t\treturn clip_image\n",
    "\n",
    "def encode_image(clip_model, clip_image):\n",
    "\timage_features = clip_model.encode_image(clip_image).type(dtype)\n",
    "\treturn image_features\n",
    "\n",
    "def similarity(text_array, text_features, image_features, top_count=1):\n",
    "        similarity = torch.zeros((1, len(text_array))).to(device)\n",
    "        for i in range(image_features.shape[0]):\n",
    "            similarity += (100.0 * image_features[i].unsqueeze(0) @ text_features.T).softmax(dim=-1)\n",
    "        similarity /= image_features.shape[0]\n",
    "\n",
    "        top_probs, top_labels = similarity.cpu().topk(top_count, dim=-1)\n",
    "        return [(text_array[top_labels[0][i].numpy()], (top_probs[0][i].numpy()*100)) for i in range(top_count)] \n",
    "\n",
    "def torch_gc():\n",
    "\twith torch.cuda.device('cuda:0'):\n",
    "\t\ttorch.cuda.empty_cache()\n",
    "\t\ttorch.cuda.ipc_collect()\n",
    "\n",
    "def rank(clip_model, image_features, text_array, top_count=1):\n",
    "\ttop_count = min(top_count, len(text_array))\n",
    "\ttext_tokens = clip.tokenize(list(text_array), truncate=True).to(device)\n",
    "\ttext_features = clip_model.encode_text(text_tokens).type(dtype)\n",
    "\ttext_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "\tsimilarity = torch.zeros((1, len(text_array))).to(device)\n",
    "\tfor i in range(image_features.shape[0]):\n",
    "\t\tsimilarity += (100.0 * image_features[i].unsqueeze(0) @ text_features.T).softmax(dim=-1)\n",
    "\tsimilarity /= image_features.shape[0]\n",
    "\n",
    "\ttop_probs, top_labels = similarity.cpu().topk(top_count, dim=-1)\n",
    "\treturn [(text_array[top_labels[0][i].numpy()], (top_probs[0][i].numpy()*100)) for i in range(top_count)]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, preprocess = load_clip_model()\n",
    "clip_model = model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_10 = Image.open('images/10.png')\n",
    "img_10_features = encode_image(model, preprocess_img(preprocess, img_10))\n",
    "\n",
    "img_30 = Image.open('images/30.png')\n",
    "img_30_features = encode_image(model, preprocess_img(preprocess, img_30))\n",
    "\n",
    "# %%markdown\n",
    "# ![title](images/10.png)\n",
    "# ![title](images/30.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model = model\n",
    "device = 'cuda'\n",
    "dtype = torch.float32\n",
    "\n",
    "text = 'a photo of a cat wearing a pink hat on a blue rug'\n",
    "text_array = text.split(' ')\n",
    "text_array = c\n",
    "text_tokens = clip.tokenize(text, truncate=True).to(device)\n",
    "text_features = clip_model.encode_text(text_tokens).type(dtype)\n",
    "\n",
    "with torch.no_grad():\n",
    "\tprint(similarity(text_array, text_features, img_10_features, len(text_array)))\n",
    "\n",
    "text_features_single  = clip_model.encode_text(text_tokens).type(dtype)\n",
    "\n",
    "text_concat = ', '.join([text, text, text, text])\n",
    "text_tokens = clip.tokenize(text_concat, truncate=True).to(device)\n",
    "\n",
    "\n",
    "text_token_list = [clip.tokenize(x, truncate=True).to(device) for x in text_array]\n",
    "empty_token = clip.tokenize('', truncate=True).to(device)\n",
    "text_feature_list = [clip_model.encode_text(t).type(dtype) for t in text_token_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.zeros_like(text_features).to(torch.float32)\n",
    "torch.not_equal(text_features, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x.shape for x in text_feature_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
